<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank">
</head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">What If?</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2025 CSCI 5541 NLP: Class Project - University of
        Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">It's NLPeak</h4>

      <div class="authors-wrapper">

        <div class="author-container">
          <div class="author-image">

            <img src="">


          </div>
          <p>

            Riandy Setiadi

          </p>
        </div>

        <div class="author-container">
          <div class="author-image">

            <img src="">

          </div>
          <p>

            Rohan Cherukuri

          </p>
        </div>

        <div class="author-container">
          <div class="author-image">

            <img src="">

          </div>
          <p>
            Stefan Hermann
          </p>
        </div>

        <div class="author-container">
          <div class="author-image">

            <img src="">

          </div>
          <p>
            Matt He
          </p>
        </div>

      </div>

      <br />

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a href="https://github.com/greenden007/what-if" target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined">
              <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://github.com/greenden007/what-if" target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined">
              <span>Code</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://github.com/greenden007/what-if" target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined">
              <span>Model Weights</span>
            </a>
          </span>
        </div>
      </div>


    </div>
  </div>









  <div class="wrapper">
    <hr>

    <h2 id="abstract">Abstract</h2>

    <!-- <p>One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained.</p> -->
    <p>Character consistenncy is a fundamental aspect of good storytelling, yet scriptwriters often stuggle to maintain
      coherent personlity trains along long narritives. And while recent advances in LLMs have shown good text
      generation capabilities, they are still lacking when it comes to character personality traits. To address this,
      our project explores wheter transformer-based models can modify existing film dialogue to shift personality
      archetypes while preseving narritive choherence. We propose a two-stage framework: a classification model that
      learns personality representations from dialouge, and a generation module that conditions text rewriting on target
      archetypes. </p>

    <hr>

    <h2 id="teaser">Methodology</h2>



    <p class="sys-img"><img src="./files/Screenshot 2025-12-04 at 7.49.25 PM.png" alt="imgname"></p>



    <p></p>

    <hr>

    <h2 id="introduction">Introduction / Background / Motivation</h2>

    <p>
      <b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no
        jargon.</b>
    </p>
    <p>
      We set out to explore whether language models can understand and modify the archtypes of characters in dialogue.
      Our goal was to develop a model that could take an existing script and adjust a character's archetype without
      breaking the flow of the story. Either for character consistency or for imagining new archtypes like if in Finding
      Nemo, Dory was smart.
    </p>

    <p>
      <b>How is it done today, and what are the limits of current practice?</b>
    </p>
    <p>
      Today, most personality-adaptive models focus on broad emotional tone or sentiment rather than story-specific
      archtypes. Additionally, most systems generate text from scratch instead of modifying existing content, which
      makes emantic presevation difficult.
    <p>

    <p>
      <b>Who cares? If you are successful, what difference will it make?</b>
    </p>
    <p>
      If successful, this sytem could become a powerful creative tool for writers and educators. It would allow authors
      to explore how stories change when characters personalities shift, and could also be a tool for adapting things
      like tutors or virtual agents and adjusting their persona to better engage different audiences.
    </p>

    <hr>

    <h2 id="approach">Approach</h2>

    <p>
      <b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything
        new in your approach?</b>
    </p>

    <p>
      Stage 1 focuses on archetype classification using a Qwen2.5-32B-Instruct model fine-tuned on 100K labeled
      dialogues. The system employs a 124-class personality classifier, oversampling low-frequency classes during
      training and running for 8 epochs using 61 movie samples, enabling effective scene-by-scene attention management.
      Stage 2 performs personality-conditioned generation with a Qwen2.5-14B-Instruct model enhanced through LoRA across
      seven target modules, including attention and MLP layers. A FAISS index supports retrieval of the three most
      similar examples (k=3) to incorporate into generation. Training for this stage uses 1000 samples over 3 epochs
      with a batch size of 8. Prompt engineering incorporates task-specific instructions with explicit archetype
      descriptions, few-shot examples retrieved via FAISS, and a structured prompt format that moves from context, to
      retrieved examples, to the final generation target.
    </p>

    <p>
      <b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
    </p>

    <p>
      Some problems that we have encountered include the evaluation portion. How do we tell if the narrative of the
      story is the same? How do we tell if the new dialogue is actually of a new archtype? The system must maintain
      narrative coherence even as a character’s personality shifts, which is addressed through a RAG-based retrieval
      mechanism that selects archetype-specific examples to guide generation. Computational limitations of large
      language models are mitigated by applying efficient LoRA fine-tuning to Qwen 2.5-14B. To overcome the shortage of
      labeled character personality data, GPT-5-nano is used to automatically label 500 characters. Finally, balancing
      personality shifts while preserving semantic content is achieved through a two-stage pipeline that cleanly
      separates personality classification from personality-conditioned generation.
    </p>

    <hr>

    <h2 id="results">Results</h2>
    <p>
      <b>How did you measure success? What experiments were used? What were the results, both quantitative and
        qualitative? Did you succeed? Did you fail? Why?</b>
    </p>
    <p>
      Evaluation includes both automatic and human metrics. Automatic metrics consist of a Personality Alignment Score,
      BERTScore, measures of contextual coherence, and perplexity. Human evaluation is conducted on 100 samples, each
      rated across four dimensions: personality match, fluency, contextual fit, and consistency of character voice. As a
      comparison point, zero-shot Claude is used as the baseline model.
    </p>
    <table>
      <thead>
        <tr>
          <th style="text-align: center"><strong>Eval Metrics</strong></th>
          <th style="text-align: center">Classification Accuracy</th>
          <th style="text-align: center">Personality Alignment Score (PAS)</th>
          <th style="text-align: center">BERTScore</th>
          <th style="text-align: center">Contextual Coherence</th>
        </tr>
      </thead>
      <tbody>
        <tr>
        <tr>
          <td style="text-align: center"><strong>Score Means</strong></td>
          <td style="text-align: center">48.9%</td>
          <td style="text-align: center">0.493</td>
          <td style="text-align: center">0.838</td>
          <td style="text-align: center">0.16</td>
        </tr>
      </tbody>
      <caption>Results Table</caption>
    </table>
    <br>

    <hr>



    <h2 id="conclusion">Conclustion and Future Work</h2>
    <p>

      How easily are your results able to be reproduced by others?
      Did your dataset or annotation affect other people's choice of research or development projects to undertake?
      Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
      What limitations does your model have? How can you extend your work for future research?</p>



    <p>Our results are reasonably reproducible because we document the two-stage pipeline, training configurations, LoRA
      modules, and retrieval setup; however, full reproduction depends on access to the same Qwen models, the annotated
      personality dataset, and the FAISS retrieval index. While our dataset and GPT-5-nano–generated annotations may not
      directly influence others’ research directions, they lower the barrier for studying personality-driven narrative
      generation by providing a structured set of archetypes and labeled examples. This could encourage future work in
      controllable storytelling, character simulation, and narrative analysis.

      There are potential societal risks. Personality-conditioned generation can be misused for creating persuasive or
      manipulative character voices, generating synthetic personas for misinformation, or reinforcing harmful
      stereotypes associated with personality archetypes. These risks can be mitigated by using transparent,
      well-documented archetype definitions, bias-auditing the dataset, restricting deployment to creative or research
      contexts, and giving users controls or disclaimers about generated content.

      The model has several limitations: it struggles with long-range narrative consistency, relies heavily on retrieval
      quality, and may overfit archetype descriptions, leading to exaggerated or caricatured personalities. It also
      depends on automated labels, which may introduce noise or bias. Future research could improve the system by
      incorporating larger or more diverse script datasets, using hierarchical memory for long-term character state
      tracking, refining personality taxonomies, collecting human-verified labels, or integrating reinforcement learning
      to optimize for multi-turn coherence and nuanced personality expression.</p>

    <hr>

  </div>



</body>

</html>